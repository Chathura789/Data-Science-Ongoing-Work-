{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6. Optimization. Programming Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT CHANGE\n",
    "import grading\n",
    "import grading_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT CHANGE\n",
    "grader = grading.Grader(assignment_key=\"Aoii7s9WRQyvONf6kwwjAw\", \n",
    "                        all_parts=[\"NCrTc\", \"JigtH\", \"HlACv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token expires every 30 min\n",
    "COURSERA_TOKEN = \"Q6qZJenThnZD6q08\"\n",
    "COURSERA_EMAIL = \"alexander.d.eric@gmail.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the **House Pricing** dataset, where you have a lot of information about the houses being sold and you aim to produce the price of the house. \n",
    "\n",
    "Firstly, let us import basic libraries (`numpy` ([docs](https://numpy.org/)) for matrix operations and `pandas` ([docs](https://pandas.pydata.org/)) for convinient dataset workaround):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Reading and Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datX=np.load('x_train.npy')\n",
    "datY=np.log(np.load('y_train.npy'))\n",
    "datX=pd.DataFrame(datX, columns=datX.dtype.names)\n",
    "datX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we manage to load the data (you can read more about the `load` [here](https://docs.scipy.org/doc/numpy/reference/generated/numpy.load.html). But it is not a necessity). We are going to use linear models to work with it, but firstly we need to come up with idea what features should we include in the model at all (which feature the price is lineary dependent on):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not forget to install seaborn. You can do that by running `pip install seaborn` in the command line locally, or simply by running the next sell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do it let us plot every feature vs the price. Firstly, we import nice plotting modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax=plt.subplots(4, 4, figsize=(16,16))\n",
    "\n",
    "for i, name in enumerate(datX.columns):\n",
    "    ax[i//4][i%4].scatter(datX[name], datY)\n",
    "    ax[i//4][i%4].set_title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us say, that we choose to work the following set of features:\n",
    "+ `bedrooms`\n",
    "+ `bathrooms`\n",
    "+ `sqft_living`\n",
    "+ `floors`\n",
    "+ `condition`\n",
    "+ `grade`\n",
    "+ `sqft_above`\n",
    "+ `sqft_basement`\n",
    "+ `long`\n",
    "+ `lat`\n",
    "\n",
    "Clear the dataset from all the other features and create:\n",
    "1. matrix $X$, all elements should be real numbers\n",
    "2. number $N$ -- number of considered houses\n",
    "3. number $m$ -- number of new features\n",
    "\n",
    "**Hint**: it is easier to clean columns from dataset (you should look [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html) for insipration) and the get a matrix with `.values`\n",
    "\n",
    "**Warning**: Please use features in the order mentioned above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here\n",
    "X=datX[['bedrooms','bathrooms','sqft_living','floors','condition','grade','sqft_above','sqft_basement','long','lat']].values\n",
    "N=X.shape[0]\n",
    "m=X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells to automatically check results of your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"NCrTc\", grading_utils.test_reader(X, N, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider that we are interested in the loss of the model we discussed in the video:\n",
    "\n",
    "+ Assume we have input data that is denoted as $\\vec{x}_1, \\vec{x}_2, \\ldots, \\vec{x}_N$\n",
    "+ House prices for this input data are known $y_1, y_2, \\ldots, y_N$\n",
    "\n",
    "We propose a **simple linear model** for this task:\n",
    "\n",
    "$$ \\hat{y}_i=w_0+w_1x_1+w_2x_2+\\ldots+w_mx_m $$\n",
    "\n",
    "As a loss function we will use the mean squared error (**MSE**):\n",
    "\n",
    "$$\n",
    "Loss(\\vec{w})=\\frac{1}{N}\\sum_{i=1}^N (y_i-\\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "### Task 2. Compute analytically the $Loss(\\vec{w})$ function\n",
    "Please, keep the signature of the function and enter the code only under `your code goes here`. \n",
    "**Attention**: try to avoid usage of `for` cycles! The easiest way to do it is by using matrix operations.\n",
    "\n",
    "_Hint_: to get nice $w_0$ coefficient it is convinient to add to the `X` matrix the column of 1 with `np.concatenate` [documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(w, X, y):\n",
    "    #your code goes here\n",
    "    lossValue = np.sum(np.power(y-np.dot(np.concatenate([np.ones((X.shape[0], 1)), X], axis=1), w), 2))/X.shape[0]\n",
    "    return lossValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells to automatically check results of your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"JigtH\", grading_utils.test_loss(loss, X, datY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Compute analyticaly the gradient of the $Loss(\\vec{w})$\n",
    "Please, enter your answer in the cell below (it should be a `markdown` cell). You can either specify each partial derivative $\\frac{\\partial Loss}{\\partial w_i}$ or $\\nabla Loss$ altogether using matrix operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4. Write a function to compute the gradient of the Loss function in the given point\n",
    "Please, keep the signature of the function and enter the code only under `your code goes here`. \n",
    "**Attention**: try to avoid usage of `for` cycles! The easiest way to do it is by using matrix operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(w_k, X, y):\n",
    "    #your code goes here\n",
    "    X1=np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n",
    "    lossGradient=-2*(X1.T.dot(y-X1.dot(w_k)))/X.shape[0]\n",
    "    return lossGradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells to automatically check your function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"HlACv\", grading_utils.test_grad(grad, X, datY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5. Write gradient descent\n",
    "How it is time to formulate the gradient descent! As you remeember, the idea here is that:\n",
    "$$\n",
    "\\vec{w}^{k+1}=\\vec{w}^{k}-\\alpha_k\\cdot \\nabla Loss(\\vec{w}^{k}\n",
    "$$\n",
    "We propose that you use constant $\\alpha_k=\\alpha$. Assume that the method should stop in two cases:\n",
    "+ if the number of iterations is to high (`maxiter`)\n",
    "+ if the length of the gradient is low enough (<`eps`) to call an extremum\n",
    "\n",
    "Please, keep the signature of the function and enter the code only under `your code goes here`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradDescent(w_init, alpha, X, y, maxiter=500, eps=1e-2):\n",
    "    losses=[]\n",
    "    weights=[w_init]\n",
    "    \n",
    "    curiter=0\n",
    "    w_k=weights[-1]\n",
    "    \n",
    "    #your code goes here\n",
    "    while curiter<maxiter and len_grad>eps:\n",
    "        \n",
    "        gradient    = grad(w_k, X, y)\n",
    "        w_k         = w_k - alpha*gradient\n",
    "        lossValue_k = loss(w_k, X, y)\n",
    "        len_grad    = np.linalg.norm(gradient)\n",
    "        weights.append(w_k)\n",
    "        losses.append(lossValue_k)\n",
    "        \n",
    "        curiter     = curiter+1\n",
    "        \n",
    "    return weights, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with several alphas and several intial values of weights. To illustrate, provide graphs for the Loss function over iterations in each case (and, optionally, the distance between weigths from one iteration to the next):\n",
    "\n",
    "(we provided all key plotting commands for you, but you can always look into [this tutorial](https://matplotlib.org/tutorials/introductory/usage.html#sphx-glr-tutorials-introductory-usage-py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'len_grad' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e975ded387e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mw_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mweights_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-f4e2cb229765>\u001b[0m in \u001b[0;36mgradDescent\u001b[0;34m(w_init, alpha, X, y, maxiter, eps)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#your code goes here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0mcuriter\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mmaxiter\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen_grad\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mgradient\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'len_grad' referenced before assignment"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "# your code here\n",
    "w_init=np.random.uniform(0, 1, X.shape[1]+1)\n",
    "alpha =  0.01\n",
    "weights_1, losses_1 = gradDescent(w_init, alpha, X2, datY, maxiter=500, eps=1e-2)\n",
    "\n",
    "alpha =  0.1\n",
    "weights_2, losses_2 = gradDescent(w_init, alpha, X2, datY, maxiter=500, eps=1e-2)\n",
    "\n",
    "alpha =  0.2\n",
    "weights_3, losses_3 = gradDescent(w_init, alpha, X2, datY, maxiter=500, eps=1e-2)\n",
    "\n",
    "losses\n",
    "plt.plot(losses_1, label=(\"Alpha = 0.01 & w_init (0,1)\"))\n",
    "plt.plot(losses_2, label=(\"Alpha =1 & w_init (0,1)\"))\n",
    "plt.plot(losses_3, label=(\"Alpha =1 & w_init (0,1)\"))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the adequacy of the model we created.\n",
    "\n",
    "Choose several (no less then five) houses (inputs in your `X` matrix) and calculte predicted prices by:\n",
    "\n",
    "$$ \\hat{y}_i=w_0+w_1x_1+w_2x_2+\\ldots+w_mx_m $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weights_3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-15ae8a8eab68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weights_3' is not defined"
     ]
    }
   ],
   "source": [
    "### your code goes here\n",
    "idx = np.random.randint(X.shape[0], size=500)\n",
    "idx\n",
    "X1 = X[idx, :]\n",
    "X2 =(X1-X.mean(axis=0))/X.std(axis=0)\n",
    "X2 = np.concatenate([np.ones((X2.shape[0], 1)), X2], axis=1)\n",
    "yhat = X2.dot(weights_3[-1])\n",
    "\n",
    "\n",
    "plt.scatter(y[idx],yhat)\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.legend()\n",
    "\n",
    "np.power(sum(np.power(y[idx]-yhat,2))/X2.shape[0],0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare predicted values with an actual answer (stored in your `y` array). Is it satisfying enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The root-mean-square error is 0.28 and actual vs predicted are a high correlation. To test the forecasting of the models, validation should be on a test data set which is not used within the training of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6. Discussion\n",
    "Answer following questions:\n",
    "1. Does your method converge at least for some alpha? If not, what might be the workaround?\n",
    "2. How does changing of the alpha influence the speed of convergence?\n",
    "3. Are the optimal weights in all convergent cases the same?\n",
    "4. How does this affect the Loss function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the standardization of data, the gradient descent method converges for values of alpha less than 0.2. For higher values of the alpha, the model tends to diverge.\n",
    "\n",
    "As seen from the above plots as alpha increases from very small values to 0.2 the speed of convergence increases.\n",
    "\n",
    "Optimal weights are identical as long as the model converges to the desired accuracy.\n",
    "\n",
    "The final loss function value of model convergence is the same as long as models are converging."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
